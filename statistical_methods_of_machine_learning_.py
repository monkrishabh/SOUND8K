# -*- coding: utf-8 -*-
"""STATISTICAL METHODS OF MACHINE LEARNING .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fPuoHdiKOMW9EmpPsAQjANqgH5YNtCsz
"""

!pip install -q kaggle

from google.colab import files
files.upload();

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

!pip install --upgrade --force-reinstall --no-deps kaggle

!kaggle datasets list

!kaggle datasets download -d chrisfilo/urbansound8k

!ls

!unzip urbansound8k.zip

!pwd

!pip install pandas
!pip install librosa

#### Extracting MFCC's For every audio file
import pandas as pd
import os
import librosa
import numpy as np

audio_dataset_path=''
dataset=pd.read_csv('UrbanSound8K.csv')
dataset.head()

def features_extractor(file):
    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') 
    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)
    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)
    
    return mfccs_scaled_features

from tqdm import tqdm
### Now we iterate through every audio file and extract features 
### using Mel-Frequency Cepstral Coefficients
extracted_features=[]
for index_num,row in tqdm(metadata.iterrows()):
    file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row["fold"])+'/',str(row["slice_file_name"]))
    final_class_labels=row["class"]
    data=features_extractor(file_name)
    extracted_features.append([data,final_class_labels])

### converting extracted_features to Pandas dataframe
extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])
extracted_features_df.head()

### Split the dataset into independent and dependent dataset
X=np.array(extracted_features_df['feature'].tolist())
y=np.array(extracted_features_df['class'].tolist())

X.shape;

### Label Encoding
y=np.array(pd.get_dummies(y))

y.shape;

### Train Test Split
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)

X_train

y

X_train.shape

X_test.shape

y_train.shape

y_test.shape

from sklearn import metrics
from sklearn.metrics import confusion_matrix, mean_squared_error, roc_auc_score,roc_curve
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.metrics import classification_report,accuracy_score

rf = 'Random Forest Classfier'
rf = RandomForestClassifier()
rf.fit(X_train,y_train)
rf_predicted = rf.predict(X_test)
rf_acc_score = accuracy_score(y_test, rf_predicted)

print("\n")
print("Accuracy of Random Forest:",rf_acc_score*100,'\n')
print(classification_report(y_test,rf_predicted))

knn= KNeighborsClassifier()
knn.fit(X_train,y_train)
y_pred_knn = knn.predict(X_test)

knn_acc_score = accuracy_score(y_test, y_pred_knn)

print("\n")
print("Accuracy of K Neighbors :",knn_acc_score*100,'\n')
print(classification_report(y_test,rf_predicted))

dtc= DecisionTreeClassifier()
dtc.fit(X_train,y_train)
y_pred_dtc = dtc.predict(X_test)

dtc_acc_score = accuracy_score(y_test, y_pred_dtc)

print("\n")
print("Accuracy of Decision Tree :",dtc_acc_score*100,'\n')
print(classification_report(y_test,rf_predicted))

model_ev = pd.DataFrame({'Model': ['K Neighbors Classifier','Decision Tree Classifier','Random Forest Classfier'],'Accuracy': [knn_acc_score*100,dtc_acc_score*100,rf_acc_score*100]})
model_ev

# Commented out IPython magic to ensure Python compatibility.
from matplotlib import pyplot as plt
# %matplotlib inline
colors = ['red','green','blue']
plt.figure(figsize=(7,3))
plt.title("Barplot representation of Different models accuracy")
plt.xlabel("Accuracy of Algorithms")
plt.ylabel("Percentage")
plt.bar(model_ev['Model'], model_ev['Accuracy'],color = colors)
plt.show()

num_labels= y.shape[1]

import keras
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout
from keras.optimizer_v1 import Adam
from keras.callbacks import TensorBoard

cnn = Sequential()
cnn.add(Conv1D(32, 2, activation='relu', input_shape=(40,1)))
cnn.add(Dropout(0.5))

cnn.add(Conv1D(64, 2, activation='relu', input_shape= (40,1)))
cnn.add(Dropout(0.5))

cnn.add(Conv1D(128, 2, activation='relu', input_shape= (40,1)))
cnn.add(Dropout(0.5))


cnn.add(Flatten())
cnn.add(Dropout(0.3))
cnn.add(Dense(64, activation='relu'))
cnn.add(Dropout(0.3))

cnn.add(Dense(num_labels, activation='sigmoid'))

cnn.summary()

cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])

epochs = 100
batch_size = 32
history = cnn.fit(X_train , y_train , verbose=1 , batch_size=batch_size , epochs=epochs ,validation_data=(X_test, y_test) )

fig, ax1 = plt.subplots(figsize= (10, 5) )
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.savefig("acc.png")
plt.show()


fig, ax1 = plt.subplots(figsize= (10, 5) )
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.savefig("loss.png")
plt.show()

test_accuracy = cnn.evaluate(X_test, y_test, verbose = 1)
print(test_accuracy[1]*100)

!cd fold1

!ls

filename="/content/SIREN.wav"
audio, sample_rate = librosa.load(filename, res_type='kaiser_fast') 
mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)
mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)

print(mfccs_scaled_features)
mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)
print(mfccs_scaled_features)
print(mfccs_scaled_features.shape)
#predicted_label=model.predict_classes(mfccs_scaled_features)
y_pred = cnn.predict(mfccs_scaled_features)
predicted_label = np.round(y_pred).astype(int)
print(predicted_label)

if(predicted_label[0][0] == 1):
	print("Air Conditioner")
elif(predicted_label[0][1] == 1):
	print("Car Horn")
elif(predicted_label[0][2] == 1):
	print("Children Playing")
elif(predicted_label[0][3] == 1):
	print("Dog Bark")	
elif(predicted_label[0][4] == 1):
	print("Drilling")	
elif(predicted_label[0][5] == 1):
	print("engine_idling")	
elif(predicted_label[0][6] == 1):
	print("Gun Shot")	
elif(predicted_label[0][7] == 1):
	print("Jack Hammer")
elif(predicted_label[0][8] == 1):
	print("Siren")
elif(predicted_label[0][9] == 1):
	print("Street Music")